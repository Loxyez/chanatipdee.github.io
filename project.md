---
layout: null
section-type: project
title: project
---

## Project


#### RNNLM and n-gram statistics
This project explores how explicitly stated n-gram distribution can be used as a set of soft constraints to direct the language model behavior during the generation without sacrificing its accuracy in assigning probability to sequences of words. We apply the technique to reduce word-level repetition (a common problematic behavior). It also improves model generalizability by incorporating statistical constraints are n-gram statistics taken from a large corpus.

[ Paper: [AAAI'18](https://www.cs.northwestern.edu/~ddowney/publications/noraset_aaai_2018.pdf) \| Links: [code](https://github.com/northanapon/seqmodel/tree/aaai18)]
